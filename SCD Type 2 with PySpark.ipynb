{"cells":[{"cell_type":"code","execution_count":null,"id":"18160d9f-9dd7-4a15-b259-55520df239a5","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["from pyspark.sql.functions import col, explode_outer, lit, when, row_number, trim\n","from pyspark.sql.types import *\n","import os\n","from datetime import datetime\n","from delta.tables import *"]},{"cell_type":"markdown","id":"135b0933-2cdb-4222-ac1e-39c829d7749f","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["##### dim_client as a SCD Type 2 using PySpark"]},{"cell_type":"code","execution_count":null,"id":"7d6b7665-9ba9-487f-94c0-a2e6c9537c99","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Select & rename columns, specify non string column types, create dataframe \n","dim_client = df \\\n","    .select('ClientCode', 'ClientName') \\\n","    .dropDuplicates()\n","\n","# Force schema, create delta table if not exists\n","DeltaTable.createIfNotExists(spark) \\\n","    .tableName('dim_client') \\\n","    .addColumn('ClientCode', StringType()) \\\n","    .addColumn('ClientName', StringType()) \\\n","    .addColumn('StartDate', TimestampType()) \\\n","    .addColumn('EndDate', TimestampType()) \\\n","    .addColumn('CurrentFlag', BooleanType()) \\\n","    .execute()\n","\n","# Metadata columns for SCD Type 2\n","source_table = dim_client \\\n","    .withColumn(\"StartDate\", lit(current_datetime).cast(\"timestamp\")) \\\n","    .withColumn(\"EndDate\", lit(None).cast(\"timestamp\")) \\\n","    .withColumn(\"CurrentFlag\", lit(\"Y\").cast(\"boolean\"))\n","\n","target_table = DeltaTable.forPath(spark, 'Tables/dim_client')"]},{"cell_type":"code","execution_count":null,"id":"22ea71eb-7a62-4a6d-80c2-c51c665b5dcf","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Insert / Update with SCD Type 2 logic\n","target_table.alias(\"target\") \\\n","    .merge(\n","        source_table.alias(\"source\"),\n","        \"target.ClientCode = source.ClientCode AND target.CurrentFlag = 'Y'\"\n","    ) \\\n","    .whenMatchedUpdate(condition = \"target.ClientName != source.ClientName\",\n","\n","                       set = {\n","                            \"EndDate\": lit(current_datetime),\n","                            \"CurrentFlag\": lit(\"N\")\n","                        }\n","    ) \\\n","    .whenNotMatchedInsert(values = {\n","        \"ClientCode\": \"source.ClientCode\",\n","        \"ClientName\": \"source.ClientName\",\n","        \"StartDate\": \"source.StartDate\",\n","        \"EndDate\": \"source.EndDate\",\n","        \"CurrentFlag\": lit(\"Y\")\n","    }\n","    ) \\\n","    .execute()\n","\n","# Insert the new versions of the records that have changed\n","changed_records = source_table.alias(\"source\").join(\n","    target_table.toDF().alias(\"target\"),\n","    (col(\"source.ClientCode\") == col(\"target.ClientCode\")) & (col(\"target.CurrentFlag\") == \"N\"),\n","    \"inner\"\n",").filter(\n","    (col(\"source.ClientName\") != col(\"target.ClientName\"))\n",").select(\n","    col(\"source.ClientCode\"),\n","    col(\"source.ClientName\"),\n","    lit(current_datetime).alias(\"StartDate\"),\n","    lit(None).cast(\"timestamp\").alias(\"EndDate\"),\n","    lit(\"Y\").cast(\"boolean\").alias(\"CurrentFlag\")\n",")\n","\n","changed_records.write.format(\"delta\").mode(\"append\").save(\"Tables/dim_client\")"]},{"cell_type":"markdown","id":"d93fdf7d-73d4-4456-a0fd-ad0d0007e1cc","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["##### Special Duplicates: removing rows with less information"]},{"cell_type":"code","execution_count":null,"id":"55ab572a-621d-4083-9d96-6fa7c4c907d6","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["from pyspark.sql.window import Window # Provides tools for creating window specifications that define how to partition and order data for window functions\n","from functools import reduce\n","\n","# Function to count non-NULL values per row\n","def count_non_nulls(*cols):\n","    return reduce(lambda a, b: a + b, [when(col(c).isNotNull(), 1).otherwise(0) for c in cols])"]},{"cell_type":"code","execution_count":null,"id":"1062ff2c","metadata":{},"outputs":[],"source":["# Select & rename columns, specify non string column types, create dataframe \n","dim_collection = df \\\n","    .select('CollectionCompany', 'CollectionAddressLine1', 'CollectionAddressLine2', 'CollectionAddressLine3', 'CollectionAddressLine4', 'CollectionPostcode', 'CollectionAlternativePlace', 'CollectionCountryCode', 'CollectionCountryName') \\\n","    .dropDuplicates()\n","\n","# Drop duplicate rows based on contact and address, and keep ones with more information in the remaining columns\n","# Create a column to count non-null values in each row\n","columns_to_check = dim_collection.columns\n","df_with_non_null_count = dim_collection.withColumn(\"NonNullCount\", count_non_nulls(*columns_to_check))\n","\n","# Define a window specification to partition by the columns we consider for duplicates\n","window_spec = Window.partitionBy(\"CollectionCompany\", \"CollectionAddressLine1\", \"CollectionPostcode\").orderBy(col(\"NonNullCount\").desc())\n","\n","# Add a row number within each partition to identify the row with the most non-null values\n","df_with_row_number = df_with_non_null_count.withColumn(\"row_number\", row_number().over(window_spec))\n","\n","# Filter to keep only the rows with row number 1 (the row with the most non-null values in each group)\n","dim_collection = df_with_row_number.filter(col(\"row_number\") == 1).drop(\"NonNullCount\", \"row_number\")"]},{"cell_type":"markdown","id":"a85c775a","metadata":{},"source":["##### dim_collection as a SCD Type 2 using PySpark"]},{"cell_type":"code","execution_count":null,"id":"5b09d780-7a7f-431b-b994-fe551da6070d","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Force schema, create delta table if not exists\n","DeltaTable.createIfNotExists(spark) \\\n","    .tableName('dim_collection') \\\n","    .addColumn('CollectionCompany', StringType()) \\\n","    .addColumn('CollectionAddressLine1', StringType()) \\\n","    .addColumn('CollectionAddressLine2', StringType()) \\\n","    .addColumn('CollectionAddressLine3', StringType()) \\\n","    .addColumn('CollectionAddressLine4', StringType()) \\\n","    .addColumn('CollectionPostcode', StringType()) \\\n","    .addColumn('CollectionAlternativePlace', StringType()) \\\n","    .addColumn('CollectionCountryCode', StringType()) \\\n","    .addColumn('CollectionCountryName', StringType()) \\\n","    .addColumn('StartDate', TimestampType()) \\\n","    .addColumn('EndDate', TimestampType()) \\\n","    .addColumn('CurrentFlag', BooleanType()) \\\n","    .execute()\n","\n","# Metadata columns\n","source_table = dim_collection \\\n","    .withColumn(\"StartDate\", lit(current_datetime).cast(\"timestamp\")) \\\n","    .withColumn(\"EndDate\", lit(None).cast(\"timestamp\")) \\\n","    .withColumn(\"CurrentFlag\", lit(\"Y\").cast(\"boolean\"))\n","\n","target_table = DeltaTable.forPath(spark, 'Tables/dim_collection')"]},{"cell_type":"code","execution_count":null,"id":"45d08b29-f3e2-400c-bf85-2d468e9056f4","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Insert / Update with SCD Type 2 logic\n","target_table.alias(\"target\") \\\n","    .merge(\n","        source_table.alias(\"source\"),\n","        \"target.CollectionCompany = source.CollectionCompany AND target.CollectionAddressLine1 = source.CollectionAddressLine1 AND target.CurrentFlag = 'Y'\"\n","    ) \\\n","    .whenMatchedUpdate(condition = \"target.CollectionAddressLine2 != source.CollectionAddressLine2 OR \"\n","                                   \"target.CollectionAddressLine3 != source.CollectionAddressLine3 OR \"\n","                                   \"target.CollectionAddressLine4 != source.CollectionAddressLine4 OR \"\n","                                   \"target.CollectionPostcode != source.CollectionPostcode OR \"\n","                                   \"target.CollectionAlternativePlace != source.CollectionAlternativePlace OR \"\n","                                   \"target.CollectionCountryCode != source.CollectionCountryCode OR \"\n","                                   \"target.CollectionCountryName != source.CollectionCountryName\",\n","\n","                       set = {\n","                            \"EndDate\": lit(current_datetime),\n","                            \"CurrentFlag\": lit(\"N\")\n","                        }\n","    ) \\\n","    .whenNotMatchedInsert(values = {\n","        \"CollectionCompany\": \"source.CollectionCompany\",\n","        \"CollectionAddressLine1\": \"source.CollectionAddressLine1\",\n","        \"CollectionAddressLine2\": \"source.CollectionAddressLine2\",\n","        \"CollectionAddressLine3\": \"source.CollectionAddressLine3\",\n","        \"CollectionAddressLine4\": \"source.CollectionAddressLine4\",\n","        \"CollectionPostcode\": \"source.CollectionPostcode\",\n","        \"CollectionAlternativePlace\": \"source.CollectionAlternativePlace\",\n","        \"CollectionCountryCode\": \"source.CollectionCountryCode\",\n","        \"CollectionCountryName\": \"source.CollectionCountryName\",\n","        \"StartDate\": \"source.StartDate\",\n","        \"EndDate\": \"source.EndDate\",\n","        \"CurrentFlag\": lit(\"Y\")\n","    }\n","    ) \\\n","    .execute()\n","\n","# Insert the new versions of the records that have changed\n","changed_records = source_table.alias(\"source\").join(\n","    target_table.toDF().alias(\"target\"),\n","    (col(\"source.CollectionCompany\") == col(\"target.CollectionCompany\")) & (col(\"source.CollectionAddressLine1\") == col(\"target.CollectionAddressLine1\")) & (col(\"target.CurrentFlag\") == \"N\"),\n","    \"inner\"\n",").filter(\n","    (col(\"source.CollectionAddressLine2\") != col(\"target.CollectionAddressLine2\")) |\n","    (col(\"source.CollectionAddressLine3\") != col(\"target.CollectionAddressLine3\")) |\n","    (col(\"source.CollectionAddressLine4\") != col(\"target.CollectionAddressLine4\")) |\n","    (col(\"source.CollectionPostcode\") != col(\"target.CollectionPostcode\")) |\n","    (col(\"source.CollectionAlternativePlace\") != col(\"target.CollectionAlternativePlace\")) |\n","    (col(\"source.CollectionCountryCode\") != col(\"target.CollectionCountryCode\")) |\n","    (col(\"source.CollectionCountryName\") != col(\"target.CollectionCountryName\"))\n",").select(\n","    col(\"source.CollectionCompany\"),\n","    col(\"source.CollectionAddressLine1\"),\n","    col(\"source.CollectionAddressLine2\"),\n","    col(\"source.CollectionAddressLine3\"),\n","    col(\"source.CollectionAddressLine4\"),\n","    col(\"source.CollectionPostcode\"),\n","    col(\"source.CollectionAlternativePlace\"),\n","    col(\"source.CollectionCountryCode\"),\n","    col(\"source.CollectionCountryName\"),\n","    lit(current_datetime).alias(\"StartDate\"),\n","    lit(None).cast(\"timestamp\").alias(\"EndDate\"),\n","    lit(\"Y\").cast(\"boolean\").alias(\"CurrentFlag\")\n",")\n","\n","changed_records.write.format(\"delta\").mode(\"append\").save(\"Tables/met_dim_collection\")"]}],"metadata":{"dependencies":{"lakehouse":{"default_lakehouse":"6ef3eb4e-9ad9-4086-95a2-b1edd79a8b8d","default_lakehouse_name":"fabukslake","default_lakehouse_workspace_id":"f21d2922-702c-4c40-9257-7356d0bc6607"}},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"synapse_widget":{"state":{},"version":"0.1"},"widgets":{}},"nbformat":4,"nbformat_minor":5}
