{"cells":[{"cell_type":"code","execution_count":null,"id":"ad4c8f62-8b0d-4713-83ce-9cb270e3b007","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["import requests\n","import json\n","import base64\n","from datetime import datetime\n","import os\n","import time"]},{"cell_type":"code","execution_count":null,"id":"7602ac6a-8435-4268-bfa1-dfad57e3fa6a","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Get client credentials from Azure Key Vault\n","client_id = mssparkutils.credentials.getSecret('AZURE_KEY_VAULT_URL','SECRET_ID') \n","client_secret = mssparkutils.credentials.getSecret('AZURE_KEY_VAULT_URL','SECRET_ID') \n","\n","# Construct client credentials string and encode in Base64\n","identifier = f\"{client_id}:{client_secret}\"\n","credentials = base64.b64encode(identifier.encode()).decode()"]},{"cell_type":"markdown","id":"1fad5500-1abd-49e8-be89-c073ee52030e","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["### Get Data from Multiple API Endpoints and Store Using Best Practice Folder Hierarchy"]},{"cell_type":"code","execution_count":null,"id":"57e6ee4e-f0f4-4ece-b5fc-b5e4f207ede3","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["personal_information = {\n","    'people' : 'URL',\n","    'disabilities' : 'URL',\n","    'maritalstatus' : 'URL',\n","    'dependants' : 'URL',\n","    'havassessment' : 'URL'\n","    }\n","\n","contact_details = {\n","    'emergencycontacts' : 'URL',\n","    'communications' : 'URL',\n","    'addresses' : 'URL'\n","    }\n","\n","bank_details = {\n","    'bankdetails' : 'URL'\n","    }\n","\n","skills_and_qualifications = {\n","    'professionalqualifications' : 'URL',\n","    'education' : 'URL',\n","    'externalworkhistory' : 'URL',\n","    'languageskills' : 'URL',\n","    'personalskills' : 'URL',\n","    'licences' : 'URL'\n","}\n","\n","employment = {\n","    'calendarassigments' : 'URL',\n","    'employments' : 'URL',\n","    'empoymentstatus' : 'URL',\n","    'empoymentcontracts' : 'URL',\n","    'smcr' : 'URL',\n","    'nationalcontractsalarylevels' : 'URL',\n","    'nationalcontractqualifications' : 'URL'\n","}\n","\n","deployment = {\n","    'persongrades' : 'URL',\n","    'locationdeployments' : 'URL',\n","    'jobdeployments' : 'URL',\n","    'deploymentsorgunitorpositions' : 'URL',\n","    'costcentredeployments' : 'URL',\n","    'reportsto' : 'URL',\n","    'approvesbytype' : 'URL'\n","}"]},{"cell_type":"code","execution_count":null,"id":"63da1794-0fbc-4320-988f-6b7d77b0d133","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# List of dictionaries \n","dicts = [personal_information, contact_details, bank_details, skills_and_qualifications, employment, deployment]"]},{"cell_type":"code","execution_count":null,"id":"e1e24bd2-17e0-488b-8db4-5a4663e0f0f6","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Mapping variable names to dictionaries\n","dict_mapping = {\n","    \"personal_information\": personal_information,\n","    \"contact_details\": contact_details,\n","    \"bank_details\": bank_details,\n","    \"skills_and_qualifications\": skills_and_qualifications,\n","    \"employment\": employment,\n","    \"deployment\": deployment\n","}"]},{"cell_type":"code","execution_count":null,"id":"fc0eafc0-da41-48a1-9d6c-8454c5a05f11","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Initilize token request (defined in Functions notebook)\n","start_time = time.time()\n","access_token = get_access_token()"]},{"cell_type":"markdown","id":"21d37394-ebfb-4f7a-af67-07c70f241c7b","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["### Data Request"]},{"cell_type":"code","execution_count":null,"id":"860267b1-db3d-4adc-a96b-9f4ca6fce148","metadata":{"advisor":{"adviceMetadata":"{\"artifactId\":\"1d33edfa-0eaa-47a4-a73b-b510f528635f\",\"activityId\":\"90a10c3d-74c4-40ea-8e8d-a25197d1c8ec\",\"applicationId\":\"application_1722243067885_0001\",\"jobGroupId\":\"10\",\"advices\":{\"error\":1}}"},"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Run series of GET request through all the defined endpoints\n","for name, dictionary in dict_mapping.items():\n","    try:\n","        # Save current date\n","        current_date = datetime.now().strftime(\"%Y%m%d\")\n","\n","        # Define output directory\n","        output_dir = f\"/lakehouse/Files/API Endpoints/{name}/{current_date}/\" \n","        \n","        # Create directory if not exists\n","        if not os.path.exists(output_dir):\n","            os.makedirs(output_dir)\n","\n","        if start_time > 260:\n","            access_token = get_access_token()\n","\n","        headers = {\n","            \"Authorization\": \"Bearer \" + str(access_token),\n","            \"Host\": \"SERVER\"\n","        }\n","\n","        for key, value in dictionary.items():\n","            try:\n","                request = requests.get(value, headers=headers)\n","                print(value)\n","                \n","                # Check if the response is successful (status code 200) and contains data\n","                if request.status_code == 200 and request.content:\n","                    json_file = request.json()\n","                    \n","                    # Add a subfolder based of key\n","                    inner_dir = f\"{output_dir}/{key}\"\n","\n","                    if not os.path.exists(inner_dir):\n","                        os.makedirs(inner_dir)\n","\n","                    # Define the output file path\n","                    output_file = f\"{inner_dir}/{current_date}_APINAME_{key}.json\"\n","                    \n","                    # Write the JSON data to the output file\n","                    with open(output_file, \"w\") as f:\n","                        json.dump(json_file, f)\n","                    \n","                    # Check the file size\n","                    file_size = os.path.getsize(output_file)\n","                    print(\"JSON data has been exported to:\", output_file)\n","                    print(\"File size:\", file_size, \"bytes\")\n","                    \n","                else:\n","                    print(f\"Error: Empty or invalid response from the server (HTTP Status Code: {request.status_code})\")\n","                    \n","            except requests.RequestException as e:\n","                print(f\"Error: Request failed - {e}\")\n","    \n","    except Exception as e:\n","        print(\"An error occurred:\", e)"]},{"cell_type":"markdown","id":"32d441b5","metadata":{},"source":["### Check of the last PULL and retry endpoint where files are missing"]},{"cell_type":"code","execution_count":null,"id":"3bdc691f","metadata":{},"outputs":[],"source":["# List of directories \n","folders = [\"personal_information\", \"contact_details\", \"bank_details\", \"identification\", \"skills_and_qualifications\", \"employment\", \"deployment\"]\n","data_list =[]\n","\n","for folder in folders:\n","    parent_path = f\"/lakehouse/Files/API Endpoints/{folder}/\"\n","    latest_folder = find_latest_date_folder(parent_path)\n","\n","    if latest_folder:\n","        parent_folder_path = os.path.join(parent_path, latest_folder)\n","\n","        items =os.listdir(parent_folder_path)\n","        subfolders = [item for item in items if os.path.isdir(os.path.join(parent_folder_path, item))]\n","\n","        subfolder_count = len(subfolders)\n","\n","        modified_path = parent_folder_path.replace(\"/lakehouse/Files/API Endpoints/\", \"\")\n","        modified_path = modified_path[:-9]\n","\n","        check_file = spark.read.csv(\"Files/Reference csv/subfolder_check.csv\", header=True) # subfolder_check.csv contains expected number of subfolders per endpoint\n","\n","        data_list.append({\n","            \"ModifiedPath\": modified_path,\n","            \"LastSubfolderCount\": subfolder_count,\n","            \"LatestFolder\": latest_folder\n","        })\n","\n","    else:\n","        print(f\"No subfolders found in {parent_path}\")\n","\n","df =  spark.createDataFrame(data_list)\n","\n","check = df.join(check_file, df.ModifiedPath == check_file.endpoint, \"inner\").drop(\"nr_of_endpoints\", \"ModifiedPath\")\n","\n","# Check if LastSubfolderCount matches expected_nr_subfolders\n","matched_count = check.filter(col(\"LastSubfolderCount\") == col(\"expected_nr_subfolders\")).count()\n","\n","if matched_count < 25:\n","    missing_folders = check.filter(col(\"LastSubfolderCount\") != col(\"expected_nr_subfolders\"))\n","    print(\"Subfolders from endpoint is missing:\")\n","    missing_folders.show(truncate=False)\n","else:\n","    missing_folders = None"]}],"metadata":{"dependencies":{"lakehouse":{"default_lakehouse":"6ef3eb4e-9ad9-4086-95a2-b1edd79a8b8d","default_lakehouse_name":"fabukslake","default_lakehouse_workspace_id":"f21d2922-702c-4c40-9257-7356d0bc6607","known_lakehouses":[]}},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"synapse_widget":{"state":{},"version":"0.1"},"widgets":{}},"nbformat":4,"nbformat_minor":5}
