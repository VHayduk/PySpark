{"cells":[{"cell_type":"markdown","id":"d64b6576-0038-472b-8af8-fb1e4fa3aafe","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["The Python library base64 provides functions to encode binary data into ASCII characters using Base64 encoding and decode Base64-encoded data back into its original binary form."]},{"cell_type":"code","execution_count":28,"id":"2b0bd473-4c04-48d2-ad9e-79eb5067feec","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2024-07-29T10:23:10.91152Z","execution_start_time":"2024-07-29T10:23:10.2467667Z","livy_statement_state":"available","normalized_state":"finished","parent_msg_id":"31d292e3-5e90-4f79-9edd-9c24b4ff31c6","queued_time":"2024-07-29T10:23:09.4691426Z","session_id":"960db8d9-7f9e-4db1-8a64-309cb46bcedf","session_start_time":null,"spark_pool":null,"state":"finished","statement_id":37,"statement_ids":[37]},"text/plain":["StatementMeta(, 960db8d9-7f9e-4db1-8a64-309cb46bcedf, 37, Finished, Available, Finished)"]},"metadata":{},"output_type":"display_data"}],"source":["import requests\n","import json\n","import base64\n","import os\n","from datetime import datetime\n","from pyspark.sql import functions as F\n","from pyspark.sql.functions import current_timestamp, date_format, col, expr"]},{"cell_type":"markdown","id":"2ae5d680-8294-4066-b332-901238179398","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["### Accessing the API "]},{"cell_type":"markdown","id":"4d3e949d-6663-4972-bfd3-f1329fa3cb0a","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["Line 7 of the code:\n","takes a string, converts it to its binary representation, encodes the binary data using Base64 encoding, and then converts the resulting Base64-encoded binary data back into a string. The resulting credentials string contains the Base64-encoded representation of the original identifier string."]},{"cell_type":"code","execution_count":null,"id":"27fa560c-e67a-48c9-9c80-666c38b53160","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Get client credentials from Azure Key Vault\n","client_id = mssparkutils.credentials.getSecret('AZURE_KEY_VAULT_URL','SECRET_ID') \n","client_secret = mssparkutils.credentials.getSecret('AZURE_KEY_VAULT_URL','SECRET_ID') \n","\n","# Construct client credentials string and encode in Base64\n","identifier = f\"{client_id}:{client_secret}\"\n","credentials = base64.b64encode(identifier.encode()).decode()\n","\n","# Token endpoint URL\n","token_endpoint = \"URL\"\n","\n","# Request body parameters\n","params = {\n","    \"scope\": \"\",\n","    \"grant_type\": \"\"\n","}\n","\n","# Request headers\n","headers = {\n","    \"Content-Type\": \"application/x-www-form-urlencoded\",\n","    \"Authorization\": \"Basic \" + credentials\n","}\n","\n","# Send HTTP POST request\n","response = requests.post(token_endpoint, data=params, headers=headers)\n","\n","# Parse response JSON\n","token_data = response.json()\n","\n","# Extract access token\n","access_token = token_data.get(\"access_token\")"]},{"cell_type":"markdown","id":"3226db9e-7505-4af0-b616-564af8fa1365","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["#### The request endpoint requires 'from' and 'to' date values in a specific format to retrieve data. These values are stored to be referenced for daily calls"]},{"cell_type":"code","execution_count":null,"id":"6cef2f88-7fa5-4698-add2-3ec6be69ca1e","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["path = \"/lakehouse/default/Files/\"\n","\n","try:\n","    newest_folder = find_latest_date_folder(path) # Function defined in Functions Notebook\n","except Exception as e:\n","    print(f\"Folder hirarchy does not exist: {e}.\")\n","    newest_folder = None\n","\n","if newest_folder is not None:\n","\n","    # If previous PULL existis, use previously stored toDate as fromDate in current GET request\n","    last_pull_path = f\"{path}/{newest_folder}/{newest_folder}_date_info.json\"\n","    df = spark.read.json(last_pull_path)\n","\n","    df = df.select(\"toDate\")\n","    last_pull_date = df.collect()\n","    from_date_str = last_pull_date[0][\"toDate\"]\n","\n","    # Get the current date/time in the required format\n","    current_datetime_df = spark.range(1).select(current_timestamp().alias(\"current_datetime\"))\n","    to_date_str = current_datetime_df.select(date_format(col(\"current_datetime\"), \"yyyy-MM-dd'T'HH:mm:ss.SSSSS\").alias(\"to_date\")).collect()[0][0]\n","\n","else:\n","    # Get the current date/time in the required format\n","    current_datetime_df = spark.range(1).select(current_timestamp().alias(\"current_datetime\"))\n","    to_date_str = current_datetime_df.select(date_format(col(\"current_datetime\"), \"yyyy-MM-dd'T'HH:mm:ss.SSSSS\").alias(\"to_date\")).collect()[0][0]\n","\n","    # Subtract 24 hours from to_date to get from_date\n","    current_datetime_df = current_datetime_df.withColumn(\"from_datetime\", expr(\"current_datetime - interval 24 hours\"))\n","    from_date_str = current_datetime_df.select(date_format(col(\"from_datetime\"), \"yyyy-MM-dd'T'HH:mm:ss.SSSSS\").alias(\"from_date\")).collect()[0][0]\n","\n","\n","# GET request for endpoint Journeys\n","endpoint = f\"URL?todate={to_date_str}&fromdate={from_date_str}\"\n","\n","headers = {\n","    \"Authorization\": \"Bearer \" + str(access_token)\n","}\n","\n","request = requests.get(endpoint, headers=headers)\n","data = request.json()"]},{"cell_type":"markdown","id":"83a195f7-1a80-4899-b372-ffa960e895ab","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["#### Save JSON to bronze layer"]},{"cell_type":"code","execution_count":null,"id":"27cba596-8d31-400d-a331-92fbd025c78c","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["try:\n","    current_date = datetime.now().strftime(\"%Y%m%d\")\n","\n","    # Define the directory for the output file\n","    output_dir = f\"/lakehouse/default/Files/Data/{current_date}\"\n","    if not os.path.exists(output_dir):\n","        os.makedirs(output_dir)\n","\n","    # Define the output file path\n","    output_file = f\"{output_dir}/{current_date}_data.json\"\n","\n","    # Write the JSON data to the output file\n","    with open(output_file, \"w\") as f:\n","        json.dump(data, f)\n","\n","    # Check the file size\n","    file_size = os.path.getsize(output_file)\n","    print(\"JSON data has been exported to:\", output_file)\n","    print(\"File size:\", file_size, \"bytes\")\n","\n","    date_info_file = f\"{output_dir}/{current_date}_date_info.json\"\n","\n","    date_info = {\n","        \"Info\": \"Data captured between\",\n","        \"fromDate\": from_date_str,\n","        \"toDate\": to_date_str\n","    }\n","\n","    with open(date_info_file, \"w\") as f:\n","        json.dump(date_info, f)\n","\n","except Exception as e:\n","    print(\"An error occurred:\", e)"]}],"metadata":{"dependencies":{"lakehouse":{"default_lakehouse":"6ef3eb4e-9ad9-4086-95a2-b1edd79a8b8d","default_lakehouse_name":"fabukslake","default_lakehouse_workspace_id":"f21d2922-702c-4c40-9257-7356d0bc6607","known_lakehouses":[{"id":"6ef3eb4e-9ad9-4086-95a2-b1edd79a8b8d"}]}},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"synapse_widget":{"state":{},"version":"0.1"},"widgets":{}},"nbformat":4,"nbformat_minor":5}
